{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Netflix Recommender experiment was one of the starting points for starting this recommender systems outburst. It's about Personalization. People and items -> videos in YouTube based on history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eg: amazon - purchase history over a period; youtube; netflix; on-demand radio (pandora) - similar songs, tastes, genre; facebook; - recommendations should be coherent.\n",
    "\n",
    "drug - target repurposing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different Approaches**\n",
    " - Popularity in general (like NYT)\n",
    " - Classification Model using features of both users and the products.\n",
    "     * will have better context\n",
    "     * will be better for limited short term users\n",
    " - Collaborative model like people who bought this also bought this\n",
    "     * co-occurence of purchases or cooccurence matrix\n",
    "         - the matrix element contains the number of people purchasing both diapers; baby-wipes\n",
    "         - then we can simply look at the matrix and give the largest numbers as the recommended products\n",
    "         - what if there are very popular items? - must be normalized else most popular items are only recommended.\n",
    "             * How is this handled?\n",
    "                 - by using **Jaccard similarity** (like *tfidf* in clustering problems) :- count the number of people i and j divided by number of people who purchased i or j.\n",
    "         - How to account for history then?\n",
    "     * To account for purchase history, weighted average of all purchases. like $$(1/2)(Swipes,diaper + Swipes,milk)$$\n",
    "         - Then, How to account for context, time, product features?\n",
    "         - Then, Cold start problem - when a new user joins?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**eg: Movie recommendation problem**\n",
    "\n",
    "Representation in matrix ratings = users * movies -> sparse matrix and empty boxes are where users have not provided ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each user will have a vector containing how much he likes each of the genre\n",
    "Each movie will have a vector containing how much of the genre it is.\n",
    "\n",
    "We can do a multiplication of vectors to determine how much the *user of this genre tastes vector* will like this *movie of this genre mix.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Rating(u,v) = <L_u, R_v>$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication of these tastes vector of users and genre vector of movies will give us the likely scores of affinity for those users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *What if we don't have these topics for users and movies?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Called the **Matrix factorization model**\n",
    "\n",
    "Just like in regression model and parameters, we will use the black squares in the matrix to estimate the L and R vectors\n",
    "\n",
    "We can compute the RSS(u,v) as follows,\n",
    "\n",
    "$$ RSS(L,R) = \\sum_{all u, v pairs} (Rating(u,v)-<L_u, R_v>)^2$$\n",
    "\n",
    "where RSS stands for *Residual sum of squares*\n",
    "- Limitation still exists for a new movie or new user? when there is no data in either the row nor the column?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the context (new user / new movie ) and these matrix factorization output (learned features from past users) - blending/combining models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not similar to classification accuracy of confusion matrix. Here it's an *imbalanced class* problem. We are more interested in what they liked rather than what they didn't like we recommended (since we already have a lot of products). We need to get closer to the best recommendation asap. Also, the cost of recommending the wrong thing might be higher than not recommending any of the possible good things as well. So, here we use **precision and recall**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision = how much garbage should i look at to get an interested item. i.e.,\n",
    "$$ precision = (#liked & shown) / (#shown) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall = how much percentage of my liked items that were actually recommended\n",
    "$$ recall = (#liked & shown) / #liked $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the products I like and only the products I like is the most optimal recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision vs recall curve. We can use **Area under the curve (AUC)** measure as a metric. Ideal optimal curve would be a curve parallel to x axis where precision will stay 1 and recall will keep on increasing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the attention span (how many products) or how much precision I want - i.e., **precision at k**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
